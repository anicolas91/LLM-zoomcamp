{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation (RAG)\n",
    "We basically first retrieve data using a search engine trained to our data, and then generate an answer based on that via LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-09-14 23:44:22--  https://raw.githubusercontent.com/alexeygrigorev/minsearch/main/minsearch.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3832 (3.7K) [text/plain]\n",
      "Saving to: ‘minsearch.py.6’\n",
      "\n",
      "minsearch.py.6      100%[===================>]   3.74K  --.-KB/s    in 0s      \n",
      "\n",
      "2024-09-14 23:44:22 (34.2 MB/s) - ‘minsearch.py.6’ saved [3832/3832]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/alexeygrigorev/minsearch/main/minsearch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import minsearch # alexeys small and fast search engine\n",
    "import requests\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup LLM model client, OLLAMA MUST BE RUNNING ON THE COMPUTER\n",
    "client = OpenAI(\n",
    "    base_url='http://localhost:11434/v1/',\n",
    "    api_key='ollama',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train search engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.Index at 0x17bc37970>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load json data directly from the url \n",
    "docs_url = 'https://github.com/DataTalksClub/llm-zoomcamp/blob/main/01-intro/documents.json?raw=1'\n",
    "docs_response = requests.get(docs_url)\n",
    "documents_raw = docs_response.json()\n",
    "\n",
    "# rearrange data a bit (add course type to each faq)\n",
    "documents = []\n",
    "for course_dict in documents_raw:\n",
    "    for doc in course_dict['documents']:\n",
    "        doc['course'] = course_dict['course'] #adding it to every faq\n",
    "        documents.append(doc)\n",
    "\n",
    "# initialize class, tell the search engine what is searchable and what are keywords\n",
    "index = minsearch.Index(\n",
    "    text_fields=['text','section','question'],\n",
    "    keyword_fields=['course']\n",
    ")\n",
    "\n",
    "#actually train the search engine\n",
    "index.fit(docs=documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aux fcns\n",
    "def search(query,filter_dict={'course': 'data-engineering-zoomcamp'}):\n",
    "    '''  \n",
    "    This function runs the already trained search engine and retrieves the top 5 results,\n",
    "    '''\n",
    "    boost = {'question': 3.0, 'section': 0.5} # what to stress on, what is more important. give it weights\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict=filter_dict,\n",
    "        boost_dict=boost,\n",
    "        num_results=5\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(query, search_results):\n",
    "    ''' \n",
    "    This function starts with a prompt template, and given the query fills the template out with the results from the search engine\n",
    "    '''\n",
    "    # we will give the llm some context\n",
    "    # Alexey mentions that this is a bit of art and science because you somewhat iterate until you find something that works for you.\n",
    "    prompt_template =  \"\"\" \n",
    "\n",
    "    You are a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database. \n",
    "    Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "    If the CONTEXT does not contain the answer, output NONE.\n",
    "\n",
    "    QUESTION: {question}\n",
    "\n",
    "    CONTEXT: {context}\n",
    "\n",
    "    \"\"\".strip() #no line breaks\n",
    "    \n",
    "    #convert search results into proper formatted context\n",
    "    context = \"\"\n",
    "    for doc in search_results:\n",
    "        context = context + f\"section: {doc['section']}\\nquestion: {doc['question']}\\nanswer:{doc['text']}\\n\\n\"\n",
    "\n",
    "\n",
    "    # we formally add the info on the prompt\n",
    "    return prompt_template.format(question=query,context=context).strip()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm(prompt,model='phi3'):\n",
    "    ''' \n",
    "    This function trains chatGPT with our prompt (with the search engine results)\n",
    "    '''\n",
    "    # train openai/chatpgt with the prompt\n",
    "    response = client.chat.completions.create(\n",
    "        model = model,\n",
    "        messages=[{'role':'user','content': prompt}]\n",
    "    )   \n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(query):\n",
    "    '''  \n",
    "    This function, given a question, finds best answers on the search engine, trains the llm with it, and returns a result\n",
    "    '''\n",
    "    # search for the question on the search engine\n",
    "    results = search(query)\n",
    "    # we create the context by basically stringing together the answers from the search engine\n",
    "    prompt = build_prompt(query, results)\n",
    "    # we train the llm (in this case Ollama) with the prompt and returns some user friendly answer\n",
    "    answer = llm(prompt)\n",
    "\n",
    "    return answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, even if classes have commenced, new students who register before the end of module one can still join without penalties but must complete homework/assignments as specified in course materials by specific deadlines after a brief orientation and introduction to class content during office hours (live or recorded from previous session).\n"
     ]
    }
   ],
   "source": [
    "q= 'The course just started. Can I still enroll? Give me an answer in 1 or 2 sentences please.'\n",
    "answer = rag(q)\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
